import streamlit as st
import pandas as pd
import altair as alt
from itertools import product
import inflect
from supabase import create_client, Client

# è¯­ä¹‰å¤„ç†å™¨
p = inflect.engine()

# -------------------------
# å·¥å…·å‡½æ•°
# -------------------------
def normalize(text):
    text = text.lower().strip()
    return p.singular_noun(text) or text

def parse_keywords(text):
    if pd.isna(text) or not str(text).strip():
        return []
    return [normalize(x) for x in str(text).split("/") if x.strip()]

def expand_keyword_group(group, term_dict):
    parts = group.split("+")
    expanded = []
    for p in parts:
        if p in term_dict:
            expanded.append(term_dict[p])
        else:
            expanded.append([normalize(p)])
    return [" ".join(x) for x in product(*expanded)]

def match_grouped_terms(term, group_str, term_dict):
    words = term.split()
    phrases = expand_keyword_group(group_str, term_dict)
    return any(all(w in words for w in phrase.split()) for phrase in phrases)

def match_any_term(term, keywords, term_dict):
    words = term.split()
    expanded = []
    for k in keywords:
        if k in term_dict:
            expanded.extend(term_dict[k])
        else:
            expanded.append(normalize(k))
    return any(k in words for k in expanded)

def classify_term(search_term, rules_row, term_dict):
    term_raw = str(search_term).lower()
    words = [normalize(w) for w in term_raw.split()]
    term_norm = " ".join(words)

    hit_keywords = ""
    hit_negation = ""

    def has_neg(field, label):
        if isinstance(field, str) and field.strip():
            for word in parse_keywords(field):
                if word in words:
                    return label + ":" + word
        return ""

    if not has_neg(rules_row.get("å¦å®šè¯æ ¹_å››çº§", ""), "å››çº§"):
        for group in parse_keywords(rules_row.get("å››çº§", "")):
            if match_grouped_terms(term_norm, group, term_dict):
                return 1, 0, 0, "å››ç±»", group, ""
    else:
        hit_negation = has_neg(rules_row.get("å¦å®šè¯æ ¹_å››çº§", ""), "å››çº§")

    if not has_neg(rules_row.get("å¦å®šè¯æ ¹_ä¸‰çº§", ""), "ä¸‰çº§"):
        for group in parse_keywords(rules_row.get("ä¸‰çº§", "")):
            if match_grouped_terms(term_norm, group, term_dict):
                return 0, 1, 0, "ä¸‰ç±»", group, ""
    else:
        hit_negation = has_neg(rules_row.get("å¦å®šè¯æ ¹_ä¸‰çº§", ""), "ä¸‰çº§")

    if not has_neg(rules_row.get("å¦å®šè¯æ ¹_äºŒçº§", ""), "äºŒçº§"):
        for token in parse_keywords(rules_row.get("äºŒçº§", "")):
            if match_any_term(term_norm, [token], term_dict):
                return 0, 0, 1, "äºŒç±»", token, ""
    else:
        hit_negation = has_neg(rules_row.get("å¦å®šè¯æ ¹_äºŒçº§", ""), "äºŒçº§")

    return 0, 0, 0, "æœªåˆ†ç±»", hit_keywords, hit_negation

def classify_all(search_df, mapping_df, rules_df, term_dict):
    merged = search_df.merge(mapping_df, on="ASIN", how="left")
    merged = merged.merge(rules_df, on="SPU", how="left")

    if "SPUè¿è¥_x" in merged.columns:
        merged = merged.rename(columns={"SPUè¿è¥_x": "SPUè¿è¥"})
    elif "SPUè¿è¥_y" in merged.columns:
        merged = merged.rename(columns={"SPUè¿è¥_y": "SPUè¿è¥"})

    merged["æœˆä»½"] = pd.to_datetime(merged["æ—¥æœŸ"], errors='coerce').dt.to_period("M").astype(str)

    merged[["äºŒçº§åŒ¹é…", "ä¸‰çº§åŒ¹é…", "å››çº§åŒ¹é…", "åˆ†ç±»çº§åˆ«", "å‘½ä¸­å…³é”®è¯", "å‘½ä¸­å¦å®šè¯æ ¹"]] = merged.apply(
        lambda row: pd.Series(classify_term(row.get("æœç´¢è¯", ""), row, term_dict)), axis=1
    )
    return merged

def group_by_operator(df):
    grouped = df.groupby(["æœˆä»½","SPUè¿è¥", "åˆ†ç±»çº§åˆ«"]).agg(
        æ›å…‰æ€»é‡=("æœç´¢æ¼æ–— - å±•ç¤ºé‡: æ€»æ•°", "sum"),
        æ›å…‰ASINæ•°=("æœç´¢æ¼æ–— - å±•ç¤ºé‡: ASIN è®¡æ•°", "sum"),
        è´­ä¹°æ€»é‡=("æœç´¢æ¼æ–— - è´­ä¹°æ¬¡æ•°: æ€»æ•°", "sum"),
        è´­ä¹°ASINæ•°=("æœç´¢æ¼æ–— - è´­ä¹°æ¬¡æ•°: ASIN è®¡æ•°", "sum")
    ).reset_index()

    for col in ["æ›å…‰æ€»é‡", "æ›å…‰ASINæ•°", "è´­ä¹°æ€»é‡", "è´­ä¹°ASINæ•°"]:
        grouped[col] = pd.to_numeric(grouped[col], errors="coerce").fillna(0)
    grouped["æ›å…‰ä»½é¢å æ¯”"] = grouped["æ›å…‰ASINæ•°"] / grouped["æ›å…‰æ€»é‡"]
    grouped["è´­ä¹°ä»½é¢å æ¯”"] = grouped["è´­ä¹°ASINæ•°"] / grouped["è´­ä¹°æ€»é‡"]
    return grouped

def plot_trend_exposure_share(df, title="æ›å…‰ä»½é¢è¶‹åŠ¿å›¾"):
    g = df.groupby(["æœˆä»½", "åˆ†ç±»çº§åˆ«"]).agg(
        æ›å…‰æ€»é‡=("æœç´¢æ¼æ–— - å±•ç¤ºé‡: æ€»æ•°", "sum"),
        æ›å…‰ASINæ•°=("æœç´¢æ¼æ–— - å±•ç¤ºé‡: ASIN è®¡æ•°", "sum")
    ).reset_index()
    g["ä»½é¢"] = g["æ›å…‰ASINæ•°"] / g["æ›å…‰æ€»é‡"]

    return alt.Chart(g).mark_line(point=True).encode(
        x="æœˆä»½:N",
        y=alt.Y("ä»½é¢:Q", axis=alt.Axis(format=".0%")),
        color="åˆ†ç±»çº§åˆ«:N",
        tooltip=["æœˆä»½", "åˆ†ç±»çº§åˆ«", alt.Tooltip("ä»½é¢", format=".2%")]
    ).properties(title=title, width=800)

def plot_trend_purchase_share(df, title="è´­ä¹°ä»½é¢è¶‹åŠ¿å›¾"):
    g = df.groupby(["æœˆä»½", "åˆ†ç±»çº§åˆ«"]).agg(
        è´­ä¹°æ€»é‡=("æœç´¢æ¼æ–— - è´­ä¹°æ¬¡æ•°: æ€»æ•°", "sum"),
        è´­ä¹°ASINæ•°=("æœç´¢æ¼æ–— - è´­ä¹°æ¬¡æ•°: ASIN è®¡æ•°", "sum")
    ).reset_index()
    g["ä»½é¢"] = g["è´­ä¹°ASINæ•°"] / g["è´­ä¹°æ€»é‡"]

    return alt.Chart(g).mark_line(point=True).encode(
        x="æœˆä»½:N",
        y=alt.Y("ä»½é¢:Q", axis=alt.Axis(format=".0%")),
        color="åˆ†ç±»çº§åˆ«:N",
        tooltip=["æœˆä»½", "åˆ†ç±»çº§åˆ«", alt.Tooltip("ä»½é¢", format=".2%")]
    ).properties(title=title, width=800)

# -------------------------
# Supabase è¿æ¥
# -------------------------
url = "https://texekgedycnthkeivbtw.supabase.co"
key = "eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6InRleGVrZ2VkeWNudGhrZWl2YnR3Iiwicm9sZSI6InNlcnZpY2Vfcm9sZSIsImlhdCI6MTc1MjIxMjU5MCwiZXhwIjoyMDY3Nzg4NTkwfQ.tcTI53XXVt7lt4AdloHHoI89Ig7zFxi50MUqagKq_Oo"
supabase: Client = create_client(url, key)

@st.cache_data(ttl=1800)
def load_supabase_data_paged(max_pages=100, page_size=5000):
    all_data = []
    for i in range(max_pages):
        start = i * page_size
        end = start + page_size - 1
        response = supabase.table("search_terms_row").select("*").range(start, end).execute()
        if not response.data:
            break
        all_data.extend(response.data)
    df = pd.DataFrame(all_data)
    df.columns = df.columns.str.strip()
    df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors='coerce')
    return df

def load_sku_mapping():
    df = pd.DataFrame(supabase.table("sku_mapping").select("*").execute().data)
    df.columns = df.columns.str.strip()
    return df.drop_duplicates(subset="ASIN")

def load_spu_rules():
    df = pd.DataFrame(supabase.table("spu_rules").select("*").execute().data)
    df.columns = df.columns.str.strip()
    return df

def load_term_library():
    df = pd.DataFrame(supabase.table("term_library").select("*").execute().data)
    df.columns = df.columns.str.strip()
    term_dict = {row["åˆ†ç±»æ ‡ç­¾"]: parse_keywords(row["å¯¹åº”è¯"]) for _, row in df.iterrows()}
    return term_dict, df

# -------------------------
# é¡µé¢å¸ƒå±€
# -------------------------
st.set_page_config("æœç´¢è¯åˆ†ç±»è¡¨ç°åˆ†æ", layout="wide")
st.title("ğŸ“Š æœç´¢è¯åˆ†ç±»è¡¨ç°åˆ†æå·¥å…·")

with st.spinner("ä» Supabase åˆ†é¡µåŠ è½½æœç´¢è¯æ•°æ®..."):
    df = load_supabase_data_paged()
    mapping_df = load_sku_mapping()
    rules_df = load_spu_rules()
    term_dict, term_df = load_term_library()
    merged = classify_all(df, mapping_df, rules_df, term_dict)

# ğŸ§¾ è¯åº“ç»´æŠ¤
st.subheader("ğŸ§¾ è¯åº“ç»´æŠ¤")
term_df_edit = st.data_editor(term_df, num_rows="dynamic", use_container_width=True)
col1, col2 = st.columns(2)
with col1:
    if st.button("ğŸ’¾ ä¿å­˜è¯åº“"):
        data = term_df_edit.to_dict(orient="records")
        supabase.table("term_library").delete().neq("åˆ†ç±»æ ‡ç­¾", "").execute()
        supabase.table("term_library").insert(data).execute()
        st.success("å·²ä¿å­˜åˆ° Supabaseï¼è¯·åˆ·æ–°é¡µé¢ç”Ÿæ•ˆã€‚")
with col2:
    st.download_button("ğŸ“¤ å¯¼å‡ºè¯åº“", term_df_edit.to_csv(index=False).encode("utf-8-sig"), file_name="term_library.csv")

# ğŸ“˜ åˆ†ç±»è§„åˆ™ç»´æŠ¤
st.subheader("ğŸ“˜ åˆ†ç±»è§„åˆ™ç»´æŠ¤")
rules_edit = st.data_editor(rules_df, use_container_width=True, num_rows="dynamic")
col3, col4 = st.columns(2)
with col3:
    if st.button("ğŸ’¾ ä¿å­˜åˆ†ç±»è§„åˆ™"):
        data = rules_edit.to_dict(orient="records")
        supabase.table("spu_rules").delete().neq("SPU", "").execute()
        supabase.table("spu_rules").insert(data).execute()
        st.success("åˆ†ç±»è§„åˆ™å·²ä¿å­˜åˆ° Supabaseï¼è¯·åˆ·æ–°é¡µé¢ç”Ÿæ•ˆã€‚")
with col4:
    st.download_button("ğŸ“¥ ä¸‹è½½åˆ†ç±»è§„åˆ™", rules_edit.to_csv(index=False).encode("utf-8-sig"), file_name="spu_rules.csv")

# ğŸ” SPU åŒ¹é…è¯æŸ¥çœ‹
st.subheader("ğŸ” æŸ¥çœ‹ SPU åŒ¹é…è¯")
selected_spu = st.selectbox("é€‰æ‹©è¦æŸ¥çœ‹çš„ SPUï¼š", rules_df["SPU"].dropna().unique())
spu_row = rules_df[rules_df["SPU"] == selected_spu].iloc[0]
with st.expander(f"ğŸ“‚ SPU: {selected_spu} çš„åˆ†ç±»è§„åˆ™"):
    st.write("**äºŒçº§å…³é”®è¯ï¼š**", parse_keywords(spu_row.get("äºŒçº§", "")))
    st.write("**ä¸‰çº§å…³é”®è¯ç»„åˆï¼š**", parse_keywords(spu_row.get("ä¸‰çº§", "")))
    st.write("**å››çº§å…³é”®è¯ç»„åˆï¼š**", parse_keywords(spu_row.get("å››çº§", "")))
    st.write("**å¦å®šè¯æ ¹_äºŒçº§ï¼š**", parse_keywords(spu_row.get("å¦å®šè¯æ ¹_äºŒçº§", "")))
    st.write("**å¦å®šè¯æ ¹_ä¸‰çº§ï¼š**", parse_keywords(spu_row.get("å¦å®šè¯æ ¹_ä¸‰çº§", "")))
    st.write("**å¦å®šè¯æ ¹_å››çº§ï¼š**", parse_keywords(spu_row.get("å¦å®šè¯æ ¹_å››çº§", "")))

# ğŸ¯ ç­›é€‰æ¡ä»¶
st.subheader("ğŸ” ç­›é€‰æ¡ä»¶")
levels = ["å››ç±»", "ä¸‰ç±»", "äºŒç±»", "æœªåˆ†ç±»"]
filters = {
    "åˆ†ç±»çº§åˆ«": st.multiselect("åˆ†ç±»çº§åˆ«", levels, default=levels[:-1]),
    "æœˆä»½": st.multiselect("æœˆä»½", merged["æœˆä»½"].dropna().unique().tolist()),
    "SPUè¿è¥": st.multiselect("SPUè¿è¥", merged["SPUè¿è¥"].dropna().unique().tolist()),
    "SPU": st.multiselect("SPU", merged["SPU"].dropna().unique().tolist()),
    "æ¿å—": st.multiselect("æ¿å—", merged["æ¿å—"].dropna().unique().tolist()),
    "äº§å“çº¿": st.multiselect("äº§å“çº¿", merged["äº§å“çº¿"].dropna().unique().tolist()),
}
kw_input = st.text_input("æœç´¢è¯åŒ…å«ï¼š")

mask = merged["åˆ†ç±»çº§åˆ«"].isin(filters["åˆ†ç±»çº§åˆ«"])
for col, val in filters.items():
    if val and col != "åˆ†ç±»çº§åˆ«":
        mask &= merged[col].isin(val)
if kw_input:
    mask &= merged["æœç´¢è¯"].str.contains(kw_input, na=False, case=False)

filtered = merged[mask]

st.info(f"âœ… å½“å‰åŠ è½½æ•°æ®é‡ï¼š{len(df)} æ¡")
st.dataframe(df['æ—¥æœŸ'].dt.to_period("M").value_counts().sort_index().rename("æ¡æ•°").reset_index().rename(columns={"index": "æœˆä»½"}))

# ğŸ“ˆ æ±‡æ€»ä¸æ˜ç»†
st.subheader("ğŸ“ˆ å„ç±»è¯è¡¨ç°æ±‡æ€»")
st.dataframe(group_by_operator(filtered), use_container_width=True)

st.subheader("ğŸ“‹ ç­›é€‰åçš„æ˜ç»†æ•°æ®")
st.dataframe(filtered[["æœç´¢è¯", "åˆ†ç±»çº§åˆ«", "å‘½ä¸­å…³é”®è¯", "å‘½ä¸­å¦å®šè¯æ ¹", "SPU", "SPUè¿è¥", "æ¿å—", "äº§å“çº¿", "æœˆä»½"] +
                      [col for col in filtered.columns if col.startswith("æœç´¢æ¼æ–—")]], use_container_width=True)
st.download_button("ğŸ“¥ ä¸‹è½½ç­›é€‰æ˜ç»†", filtered.to_csv(index=False).encode("utf-8-sig"), file_name="ç­›é€‰æœç´¢è¯æ˜ç»†.csv")

# ğŸ“Š è¶‹åŠ¿å›¾
st.subheader("ğŸ“Š æ›å…‰ä»½é¢è¶‹åŠ¿å›¾")
st.altair_chart(plot_trend_exposure_share(filtered), use_container_width=True)

st.subheader("ğŸ›’ è´­ä¹°ä»½é¢è¶‹åŠ¿å›¾")
st.altair_chart(plot_trend_purchase_share(filtered), use_container_width=True)
